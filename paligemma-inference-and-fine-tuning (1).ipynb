{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U accelerate bitsandbytes transformers==4.41.1\n!pip install datasets -q\n!pip install peft -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-07T19:13:18.183741Z","iopub.execute_input":"2024-06-07T19:13:18.184100Z","iopub.status.idle":"2024-06-07T19:14:13.009645Z","shell.execute_reply.started":"2024-06-07T19:13:18.184070Z","shell.execute_reply":"2024-06-07T19:14:13.008467Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n     ","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:15:28.596624Z","iopub.execute_input":"2024-06-07T19:15:28.597019Z","iopub.status.idle":"2024-06-07T19:15:28.879579Z","shell.execute_reply.started":"2024-06-07T19:15:28.596979Z","shell.execute_reply":"2024-06-07T19:15:28.878578Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f12e3f6f394f6fbdb947cb1e6fd16d"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:15:36.726494Z","iopub.execute_input":"2024-06-07T19:15:36.727378Z","iopub.status.idle":"2024-06-07T19:15:53.482110Z","shell.execute_reply.started":"2024-06-07T19:15:36.727340Z","shell.execute_reply":"2024-06-07T19:15:53.481174Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-07 19:15:42.250271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-07 19:15:42.250375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-07 19:15:42.380210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_id = \"google/paligemma-3b-mix-224\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:26:58.725359Z","iopub.execute_input":"2024-06-07T18:26:58.726450Z","iopub.status.idle":"2024-06-07T18:26:58.731645Z","shell.execute_reply.started":"2024-06-07T18:26:58.726407Z","shell.execute_reply":"2024-06-07T18:26:58.730058Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:26:59.700080Z","iopub.execute_input":"2024-06-07T18:26:59.700446Z","iopub.status.idle":"2024-06-07T18:28:16.677451Z","shell.execute_reply.started":"2024-06-07T18:26:59.700418Z","shell.execute_reply":"2024-06-07T18:28:16.676312Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71cb0f0b16b64da2ace616383fc89953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949d2b42ef2645f09e59beb46d9538f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee57059a9db34a1d8e5fd59ee2ddf8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9ade549994497c8a5d086a74a2a29d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e167582689bc4e2a8a45004900b12fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e0a4fe21e8470c9360c94e4d54b8fc"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9ff528ce0547409f39a66789c2593a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d60bae6d6ac4812877ffa2163ac573b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67699af0c0824f35857002435e7d5ccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0575c767a26a402bb6a369943dea9659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e46e14e6d2419b839fec43da7a6ce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779650b97e2e4070a53d63c6c50f3ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e02c0ab16f4a45987f71a2d6f53506"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b03fd7d3d3f43efadec89a89c1f216e"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"What color of car is this ?\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:23.831040Z","iopub.execute_input":"2024-06-07T18:32:23.831440Z","iopub.status.idle":"2024-06-07T18:32:23.835660Z","shell.execute_reply.started":"2024-06-07T18:32:23.831409Z","shell.execute_reply":"2024-06-07T18:32:23.834737Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\n\nimage_file = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:24.004826Z","iopub.execute_input":"2024-06-07T18:32:24.005128Z","iopub.status.idle":"2024-06-07T18:32:24.009283Z","shell.execute_reply.started":"2024-06-07T18:32:24.005101Z","shell.execute_reply":"2024-06-07T18:32:24.008424Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"raw_image = Image.open(requests.get(image_file, stream=True).raw)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:24.775433Z","iopub.execute_input":"2024-06-07T18:32:24.775851Z","iopub.status.idle":"2024-06-07T18:32:25.068179Z","shell.execute_reply.started":"2024-06-07T18:32:24.775819Z","shell.execute_reply":"2024-06-07T18:32:25.067238Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\ninputs = processor(prompt, raw_image, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:25.359844Z","iopub.execute_input":"2024-06-07T18:32:25.360439Z","iopub.status.idle":"2024-06-07T18:32:25.374933Z","shell.execute_reply.started":"2024-06-07T18:32:25.360408Z","shell.execute_reply":"2024-06-07T18:32:25.374060Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\noutput = model.generate(**inputs, max_new_tokens=20)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:26.224935Z","iopub.execute_input":"2024-06-07T18:32:26.225761Z","iopub.status.idle":"2024-06-07T18:32:27.285124Z","shell.execute_reply.started":"2024-06-07T18:32:26.225723Z","shell.execute_reply":"2024-06-07T18:32:27.284331Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# # Instruct the model to create a caption in Spanish\n# # prompt = \"caption es\"\n# model_inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\")\n# input_len = model_inputs[\"input_ids\"].shape[-1]\n\n# with torch.inference_mode():\n#     generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n#     generation = generation[0][input_len:]\n#     decoded = processor.decode(generation, skip_special_tokens=True)\n#     print(decoded)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:28:55.845162Z","iopub.execute_input":"2024-06-07T18:28:55.845539Z","iopub.status.idle":"2024-06-07T18:28:55.850037Z","shell.execute_reply.started":"2024-06-07T18:28:55.845510Z","shell.execute_reply":"2024-06-07T18:28:55.849031Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:30.484981Z","iopub.execute_input":"2024-06-07T18:32:30.485346Z","iopub.status.idle":"2024-06-07T18:32:30.495460Z","shell.execute_reply.started":"2024-06-07T18:32:30.485316Z","shell.execute_reply":"2024-06-07T18:32:30.494563Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([[257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n         257152, 257152, 257152, 257152,      2,   1841,   2881,    576,   1226,\n            603,    736,   1654,    108,   8796,      1]])"},"metadata":{}}]},{"cell_type":"code","source":"print(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:34.275222Z","iopub.execute_input":"2024-06-07T18:32:34.276198Z","iopub.status.idle":"2024-06-07T18:32:34.282313Z","shell.execute_reply.started":"2024-06-07T18:32:34.276148Z","shell.execute_reply":"2024-06-07T18:32:34.281313Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\nblue\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom transformers import BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:32:36.900142Z","iopub.execute_input":"2024-06-07T18:32:36.900760Z","iopub.status.idle":"2024-06-07T18:32:36.905011Z","shell.execute_reply.started":"2024-06-07T18:32:36.900727Z","shell.execute_reply":"2024-06-07T18:32:36.903944Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map= {\"\":0}\n)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:31:08.355425Z","iopub.execute_input":"2024-06-07T18:31:08.356245Z","iopub.status.idle":"2024-06-07T18:31:50.444725Z","shell.execute_reply.started":"2024-06-07T18:31:08.356210Z","shell.execute_reply":"2024-06-07T18:31:50.443837Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e019b6dfc6d841559461ebe38b75fb20"}},"metadata":{}}]},{"cell_type":"markdown","source":"#  fine-tuning ","metadata":{}},{"cell_type":"code","source":"import os\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments, Trainer\nimport torch\nfrom peft import get_peft_model, LoraConfig","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:15:53.705560Z","iopub.execute_input":"2024-06-07T19:15:53.706453Z","iopub.status.idle":"2024-06-07T19:15:55.979670Z","shell.execute_reply.started":"2024-06-07T19:15:53.706420Z","shell.execute_reply":"2024-06-07T19:15:55.978849Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ds = load_dataset(\"HuggingFaceM4/VQAv2\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:15:58.128679Z","iopub.execute_input":"2024-06-07T19:15:58.129893Z","iopub.status.idle":"2024-06-07T19:36:14.689400Z","shell.execute_reply.started":"2024-06-07T19:15:58.129857Z","shell.execute_reply":"2024-06-07T19:36:14.688445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9dc64f8ac3447a91a9dd57345a275a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1336afd5c414d94aca27ac36371b65b"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54b4fe50a2c4e9cbf4de5dc3313fc86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f4c5de50484f0086f1398ecbaf06e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.97M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26989317e8d44a35a4973864d5d8027b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a5f1eee5274d448cd67056b423c319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/10.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cebe3bb3dee84170b7594c06e6067884"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ab9d396d674dbb96962397a9872351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890a7d1559fa45b48906b11845ab8e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5788f6c2f3a42e58a448e7e9295dc38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a922819d6864372a3bd101ae8badbe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e688c4f7f348cda027d27145836f16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating testdev split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706b57ad93ac4ecc9f7903f51b4cbdbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a92e30ab0dc422cb97590a6e8480757"}},"metadata":{}}]},{"cell_type":"code","source":"\ncols_remove = [\"question_type\", \"answers\", \"answer_type\", \"question_id\"]\nds = ds.remove_columns(cols_remove)\nds = ds.train_test_split(test_size=0.1)\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:40:15.656853Z","iopub.execute_input":"2024-06-07T19:40:15.657726Z","iopub.status.idle":"2024-06-07T19:40:15.768080Z","shell.execute_reply.started":"2024-06-07T19:40:15.657692Z","shell.execute_reply":"2024-06-07T19:40:15.766723Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cols_remove \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols_remove\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:367\u001b[0m, in \u001b[0;36mDatasetDict.remove_columns\u001b[0;34m(self, column_names)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: dataset\u001b[38;5;241m.\u001b[39mremove_columns(column_names\u001b[38;5;241m=\u001b[39mcolumn_names) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:367\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2211\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2209\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2213\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2214\u001b[0m     )\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2217\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n","\u001b[0;31mValueError\u001b[0m: Column name ['question_id', 'question_type', 'answers', 'answer_type'] not in the dataset. Current columns in the dataset: ['multiple_choice_answer', 'image_id', 'question', 'image']"],"ename":"ValueError","evalue":"Column name ['question_id', 'question_type', 'answers', 'answer_type'] not in the dataset. Current columns in the dataset: ['multiple_choice_answer', 'image_id', 'question', 'image']","output_type":"error"}]},{"cell_type":"code","source":"\nfrom transformers import PaliGemmaProcessor","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:40:19.693556Z","iopub.execute_input":"2024-06-07T19:40:19.694407Z","iopub.status.idle":"2024-06-07T19:40:19.698357Z","shell.execute_reply.started":"2024-06-07T19:40:19.694373Z","shell.execute_reply":"2024-06-07T19:40:19.697520Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 2. Load Model\nmodel_id = \"google/paligemma-3b-pt-224\"\nprocessor = PaliGemmaProcessor.from_pretrained(model_id)\ndevice = \"cuda\"\nimage_token = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:40:20.832906Z","iopub.execute_input":"2024-06-07T19:40:20.833615Z","iopub.status.idle":"2024-06-07T19:41:39.642919Z","shell.execute_reply.started":"2024-06-07T19:40:20.833583Z","shell.execute_reply":"2024-06-07T19:41:39.641909Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b9527d1be254f028b491b7a0ae3300f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4548634170c476c82924e9a2d44a3f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113460096d114034a75afff79d1d2148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13a93e456384ad4bada37e1016fe7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dde55e0e11340319c3c7f0ac516035a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604bff1bd26344398c036183d28a7e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ab77deafcb440179365c52c6a12785c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ba746466ae9455d8f4a05e5c31d39cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"081773a4956449baafcae879532dcde4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1cf172927cc46f4818694fe7276437d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01979a4bd87642938ae648e4682894e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a634d1e1a7314fcb9766054202798057"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a92eacef08548c986ec768cccce080f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2022b022a4421a88bd6241707b2b3a"}},"metadata":{}}]},{"cell_type":"code","source":"for param in model.vision_tower.parameters():\n    param.requires_grad = False \nfor param in model.multi_modal_projector.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:41:54.105746Z","iopub.execute_input":"2024-06-07T19:41:54.106138Z","iopub.status.idle":"2024-06-07T19:41:54.113123Z","shell.execute_reply.started":"2024-06-07T19:41:54.106106Z","shell.execute_reply":"2024-06-07T19:41:54.112179Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Loading Quantised Model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_type=torch.bfloat16\n)\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:41:54.932772Z","iopub.execute_input":"2024-06-07T19:41:54.933427Z","iopub.status.idle":"2024-06-07T19:41:54.941288Z","shell.execute_reply.started":"2024-06-07T19:41:54.933395Z","shell.execute_reply":"2024-06-07T19:41:54.940372Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4bit_compute_type']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n                                                          quantization_config=bnb_config,\n                                                          device_map={\"\": 0})\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:41:56.452638Z","iopub.execute_input":"2024-06-07T19:41:56.453477Z","iopub.status.idle":"2024-06-07T19:42:03.992164Z","shell.execute_reply.started":"2024-06-07T19:41:56.453442Z","shell.execute_reply":"2024-06-07T19:42:03.991106Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ae9645981e4c339eb94c181a2f8a56"}},"metadata":{}},{"name":"stdout","text":"trainable params: 11,298,816 || all params: 2,934,765,296 || trainable%: 0.3850\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3. Fine-tuning the model\ndef collate_fn(examples):\n    texts = [\"answer \" + example[\"question\"] for example in examples]\n    labels = [example['multiple_choice_answer'] for example in examples]\n    images = [example[\"image\"].convert(\"RGB\") for example in examples]\n    tokens = processor(text=texts, images=images, suffix=labels,\n                       return_tensors=\"pt\", padding=\"longest\",\n                       tokenize_newline_separately=False)\n    tokens = tokens.to(torch.bfloat16).to(device)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:42:13.754264Z","iopub.execute_input":"2024-06-07T19:42:13.754621Z","iopub.status.idle":"2024-06-07T19:42:13.760913Z","shell.execute_reply.started":"2024-06-07T19:42:13.754593Z","shell.execute_reply":"2024-06-07T19:42:13.759914Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    num_train_epochs=2,\n    remove_unused_columns=False,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=2,\n    learning_rate=2e-5,\n    weight_decay=1e-6,\n    adam_beta2=0.999,\n    logging_steps=100,\n    optim=\"adamw_hf\",\n    save_strategy=\"steps\",\n    save_steps=1000,\n    push_to_hub=True,\n    save_total_limit=1,\n    output_dir=\"paligemma_vqav2\",\n    bf16=False,\n    tf32=False ,\n    dataloader_pin_memory=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:42:16.145267Z","iopub.execute_input":"2024-06-07T19:42:16.145600Z","iopub.status.idle":"2024-06-07T19:42:16.190436Z","shell.execute_reply.started":"2024-06-07T19:42:16.145575Z","shell.execute_reply":"2024-06-07T19:42:16.189679Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    train_dataset=train_ds,\n    data_collator=collate_fn,\n    args=args\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:42:21.452795Z","iopub.execute_input":"2024-06-07T19:42:21.453552Z","iopub.status.idle":"2024-06-07T19:42:57.331701Z","shell.execute_reply.started":"2024-06-07T19:42:21.453521Z","shell.execute_reply":"2024-06-07T19:42:57.330195Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240607_194233-ymn2yg7b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adivarpe/huggingface/runs/ymn2yg7b' target=\"_blank\">paligemma_vqav2</a></strong> to <a href='https://wandb.ai/adivarpe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adivarpe/huggingface' target=\"_blank\">https://wandb.ai/adivarpe/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adivarpe/huggingface/runs/ymn2yg7b' target=\"_blank\">https://wandb.ai/adivarpe/huggingface/runs/ymn2yg7b</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[1;32m      4\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39margs\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1876\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1883\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1430, in forward\n    return self.base_model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 179, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/paligemma/modeling_paligemma.py\", line 495, in forward\n    loss = loss_fct(flat_logits, flat_labels)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1179, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 532.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 159.06 MiB is free. Process 3275 has 14.59 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 270.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1430, in forward\n    return self.base_model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 179, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/paligemma/modeling_paligemma.py\", line 495, in forward\n    loss = loss_fct(flat_logits, flat_labels)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1179, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 532.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 159.06 MiB is free. Process 3275 has 14.59 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 270.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:09:10.005159Z","iopub.execute_input":"2024-06-07T19:09:10.005528Z","iopub.status.idle":"2024-06-07T19:09:10.032594Z","shell.execute_reply.started":"2024-06-07T19:09:10.005491Z","shell.execute_reply":"2024-06-07T19:09:10.031367Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"device_map={'':torch.cuda.current_device()}","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:07:20.724417Z","iopub.execute_input":"2024-06-07T19:07:20.725025Z","iopub.status.idle":"2024-06-07T19:07:20.732032Z","shell.execute_reply.started":"2024-06-07T19:07:20.724993Z","shell.execute_reply":"2024-06-07T19:07:20.730987Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"device_map","metadata":{"execution":{"iopub.status.busy":"2024-06-07T19:07:27.314371Z","iopub.execute_input":"2024-06-07T19:07:27.315110Z","iopub.status.idle":"2024-06-07T19:07:27.322311Z","shell.execute_reply.started":"2024-06-07T19:07:27.315077Z","shell.execute_reply":"2024-06-07T19:07:27.321228Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"{'': 0}"},"metadata":{}}]},{"cell_type":"code","source":"\n# 4. Save the model in HuggingFace\ntrainer.push_to_hub('riphunter7001x/paligemma_VQAv2')","metadata":{},"execution_count":null,"outputs":[]}]}